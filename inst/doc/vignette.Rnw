\batchmode
\makeatletter
\def\input@path{{/Users/florianoswald/git/RcppSimpleTensor/inst/doc//}}
\makeatother
\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\usepackage[a4paper]{geometry}
\geometry{verbose,lmargin=2cm,rmargin=2cm}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{color}
\usepackage{float}
\usepackage{url}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{natbib}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{matrix}

\makeatother

\begin{document}

\title{Introduction to \textbf{RcppSimpleTensor}}


\author{Thibaut Lamadon and Florian Oswald\\
\url{https://github.com/tlamadon/RcppSimpleTensor}}

\maketitle
<<setup,echo=FALSE,results='hide',message=FALSE>>=
knit_hooks$set(par=function(before,options,envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=1,cex.axis=1,mgp=c(2,.7,0),tcl=-0.3,las=1)},crop=hook_pdfcrop)
opts_chunk$set(fig.path='lyx-figures/',fig.align='center',fig.show='hold',out.width='.49\\linewidth',tidy=TRUE)
@


\section{Introduction}

In this vignette we will introduce the \textbf{RcppSimpleTensor }package
by some examples. For information regarding installation, please refer
to the website (above). We will first demonstrate usage with the help
of a very simple example that permits a graphical representation.
Then we will present an application that uses \textbf{RcppSimpleTensor},
where we want to find an approximatin to the integral of a multivariate
function. 

\tableofcontents{}


\section{Usage}

Tensor algebra is a convenient way to deal with high-dimensional objects.
In a way, it is like matrix multiplication for arrays of general dimension
N-D. Note, however, that the operations one can perform with the package
are \textbf{not restricted to be matrix multiplication only}, but
\textbf{any kind of mathematical operation} \emph{along a given index}
of an array. In this section we demonstrate some ways to use the library
by operating on three arrays \texttt{A}, \texttt{b} and \texttt{B},
defined as follows:

<<usage-data>>=
A <- array(1:8,dim=c(2,2,2)) 
b <- array(1,dim=c(2,1)) 
B <- array(1,dim=c(2,2)) 
print(A)
print(b)
print(B)
@


\subsection{Multidimensional Multiplication}

We will use the two main functions in \textbf{RcppSimpleTensor}, \texttt{tensorFunction}
and \texttt{TI}, to show different forms of matrix multiplication.
The two functions perform the same tasks, the difference is that \texttt{TI}
is for inline use, and \texttt{tensorFunction} needs to be defined
before usage. First of all, notice that array \texttt{A} can be visualized
as a cube (see figure \ref{fig:The-array-a}). 
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
[back line/.style={densely dotted},cross line/.style={preaction={draw=white, -,line width=6pt}}] 
\matrix (m) [matrix of math nodes,row sep=3em, column sep=5em,text height=1.5ex,text depth=0.25ex]{& 5  & & 7 \\ 1 & & 3 \\ & 6 & & 8 \\ 2 & & 4 \\ }; 
\path[-] (m-1-2) edge (m-1-4) edge (m-2-1) edge [back line] (m-3-2) (m-1-4) edge (m-3-4) edge (m-2-3) (m-2-1) edge [cross line] (m-2-3) edge (m-4-1) (m-3-2) edge [back line] (m-3-4) edge [back line] (m-4-1) (m-4-1) edge (m-4-3) (m-3-4) edge (m-4-3) (m-2-3) edge [cross line] (m-4-3); 
\end{tikzpicture}
\end{center}

\caption{Array \texttt{A\label{fig:The-array-a}}}
\end{figure}
 We will now perform operations along the 3 different indices of \texttt{A},
and we will denote those indices \texttt{i} (vertical direction),
\texttt{j} (horizontal direction) and \texttt{k} (in-depth direction).
All of our examples are a form of tensor reduction, i.e. we operate
along a certain dimension of an array to reduce the array in this
dimension. In terms of figure \ref{fig:The-array-a}, we would flatten
the cube along a certain index, applying a certain operation to the
elements. Needless to say, the gains from using \textbf{RcppSimpleTensor}
get larger as the dimension of a grows, but we will go with this easy
example for illustration.


\subsubsection{Matrix multiply \texttt{A} with \texttt{b} along index \texttt{i}}

As a first task, we want to multiply \texttt{A} with vector \texttt{b}
along their common index \texttt{i}, that is, along the vertical direction
of \texttt{A}. This means our result will still have indices\texttt{
j} and \texttt{k}, as we only ``flattened-out'' \texttt{A} along
dimension \texttt{i}:

<<usage1,message=FALSE,tidy=FALSE>>=
library(RcppSimpleTensor)
# 1. define tensor function
tensFunc <- tensorFunction( R[k,j] ~ A[i,j,k] * b[i] )
# 2. and call it
y <- tensFunc( A, b)
y
# or create the inline tensor
TI <- createInlineTensor()
y <- TI( A[i,j,k] * b[i], k + j)
y
@

Notice that this is like looking at the cube from above and performing
the operation \texttt{A{[} ,j,k{]} {*} b} at each pair of coordinates
(j,k). If we let $y_{i,j}$ be the element in row $i$, column $j$,
we obtained 
\begin{eqnarray*}
y_{1,1} & = & A_{1,1,1}b_{1}+A_{2,1,1}b_{2}=1\cdot1+2\cdot1=3\\
y_{2,1} & = & A_{1,2,1}b_{1}+A_{2,2,1}b_{2}=3\cdot1+4\cdot1=7\\
y_{1,2} & = & A_{1,1,2}b_{1}+A_{2,1,2}b_{2}=5\cdot1+6\cdot1=11\\
y_{2,2} & = & A_{1,2,2}b_{1}+A_{2,2,2}b_{2}=7\cdot1+8\cdot1=15
\end{eqnarray*}
In (Einstein) tensor notation%
\footnote{The tensor notation is named after Einstein if one dispenses with
the summation sign. The convention is that one is summing over the
index that disappears from the resulting left hand side. See \url{http://en.wikipedia.org/wiki/Einstein_notation}.%
}, our operation is defined as 
\begin{eqnarray*}
y_{j,k} & = & A_{i,j,k}b_{i}\\
 & = & \sum_{i}A_{i,j,k}b_{i}
\end{eqnarray*}



\subsubsection{Matrix multiply along \texttt{j}}

In exact analogy to above, we can assign \texttt{b} to a different
(matching!) index of \texttt{A} -- in this case \texttt{j}. We want
to do 
\begin{eqnarray*}
y_{i,k} & = & \sum_{j}A_{i,j,k}b_{j}
\end{eqnarray*}
Thus, we obtain an \texttt{ni} by \texttt{nk} array, where \texttt{nx
}is the number of elements in dimension \texttt{x}

<<usage2>>=
TI( A[i,j,k] * b[j], k + i)
@


\subsubsection{Matrix multiply with array}

Now we do the same with 2 dimensional array \texttt{B}. Much in the
same way, we specify common indices, and give the index we wish to
keep in the result to \texttt{TI()}. In terms of tensor notation,
we want to do 
\begin{eqnarray*}
y_{k} & = & \sum_{i}\sum_{j}A_{i,j,k}B_{i,j}
\end{eqnarray*}


<<usage3>>=
TI( A[i,j,k] * B[i,j], k)
@


\subsection{Other Multidimensional Operations}

As mentioned above, you are not restricted to do multiplication. In
general, the functions will accept any binary operator. Let's see
two examples. First, let's take \texttt{A} to the power of \texttt{b}
along index \texttt{k}:
\begin{eqnarray*}
y_{i,j} & = & \sum_{k}\left(A_{i,j,k}\right)^{b_{k}}
\end{eqnarray*}
<<other1>>=
TI( A[i,j,k] ^ b[k], i + j ) 
@

Secondly, let's sum up the cosines of elements of \texttt{A} and \texttt{B},
keeping only dimension \texttt{k}:
\begin{eqnarray*}
y_{k} & = & \sum_{i}\sum_{k}\cos\left(A_{i,j,k},B_{i,j}\right)
\end{eqnarray*}
<<other2>>=
TI( cos(A[i,j,k],B[i,j]), k ) 
@


\section{Under the hood}

In this section we'll present more about what is happening under the
hood of RcppSimpleTensor. This is for users that want to understand
better what exactly is going on in this package.


\subsection{Expression parsing and compilation}

TensorFunction and TI take the expression, extract the relevant indices
and then generates a C++ file. The C++ file is then compiled. The
source file and the compiled library are located in the .tensor folder
in the working directory. You can go to that folder and see their
content. Additionally, if you call the tensorFunction with the verbose
argument, this will display all of the compilation steps.

After the compilation is done, the library is linked to the R environment
and wrapped into an R function that correctly deals with the different
dimensions.


\subsection{Caching}

Obviously the compilation phase takes quite a long time and is unnecessary
if the tensor expression has not changed. For that reason RcppSimpleTensor
has a caching mechanism. When tensorFunction is called, a hash is
created from the tensor expression. This hash is used to create the
C++ file and the library. Before compiling the tensor, the function
checks if such a library already exists locally and if it does, it
links to it.

You can see how much faster the second call for a similar tensor is:

<<caching1,message=FALSE,tidy=FALSE>>=
# 1. define tensor function, it will need to create the source and compile
system.time(tensFunc <- tensorFunction( R[k,j] ~ A[i,j,k] * b[i,n] ,cache=FALSE))
# 2. call it again, it uses the cached library
system.time(tensFunc <- tensorFunction( R[k,j] ~ A[i,j,k] * b[i,n] ))
@

This caching mechanism will also work for the TI() function. However
linking is itself slow compared to just a function call. For that
reason, to make TI() as fast as possible we use another caching mechanism.
The TI context stores a list of all TI expressions. When TI gets called,
it looks in this list if the tensor called has already been linked
to the R context. If that is the case, we don't want to load the library
again, it's already there. 

As a demo, we are going to make 3 calls to TI, the first one will
require compilation, the second one will require a linking, the third
one will use level-2 caching. Let's compare the speeds:

<<caching2,message=FALSE,tidy=FALSE>>=
# first call
system.time(TI(A[i,j,k] * b[i,n],j+k))
# second call
system.time(TI(A[i,j,k] * b[i,n],j+k))
@


\subsection{Setting compiler flags for better performance}

By default your compiler flags won't have the -O3 flag which will
give you the best performance. To activate that create a MakeVars
file in your \textasciitilde{}/.R/ directory and add the following
to it:

\begin{lstlisting}
CXXFLAGS = -O3
\end{lstlisting}


That will significantly increase the performances of your tensor expressions!


\section{Application: Evaluation of multidimensional B-splines}

\textbf{Disclaimer:} This section is likely to be useful only if you
have some prior knowledge about B-spline approximation. 

We will use observational data to approximate a multivariate function
(or a \emph{data generating process}) on a space of tensor products
of univariate B-splines. The function we want to approximte is defined
as
\begin{eqnarray*}
f:X\times Y\times Z & \mapsto & \mathbb{R}\\
f(x,y,z) & = & \left(x+y-5\right)^{2}+(z-5)^{2}
\end{eqnarray*}
and $X,Y,Z\subset\mathbb{R}$. We will get data on $f$ at a grid
of points $\left\{ x_{i},y_{j},z_{k}\right\} $, and use this to compute
an approximant to $f$. The coefficients obtained from the approximation
procedure can in conjunction with the basis be used to approximate
the function at an arbitrary set of points (in the approximation domain).
We will use this to perform integration over one, and then 2 dimensions
of this function with quadrature methods, i.e. we want to approximate
\begin{eqnarray*}
E_{Y}\left[f(x,y,z)\right] & = & \int f(x,y,z)g(y)dy\\
E_{Z}\left[f(x,y,z)\right] & = & \int f(x,y,z)s(z)dz\\
E_{YZ}\left[f(x,y,z)\right] & = & \int f(x,y,z)s(z)g(y)dzdy
\end{eqnarray*}
where $g,s$ are the pdf of $y,z$ respectively. The advantages of
\textbf{RcppSimpleTensor} are that we can write our code in a way
which is very close to the mathematical expression, and we have a
way to easily increase the number of dimensions with little additional
cost. 


\subsection{Data Setup}

The general idea is to estimate the $f$ at relatively few grid points
(\texttt{num.x}, \texttt{num.y} and \texttt{num.z}), to then be able
to obtain a function value at an arbitrary point. Imagine a setup
where calculating $f$ is costly, such that we want \texttt{num.x
+ num.y + num.z} to be small.

<<spline-setup,tidy=FALSE,message=FALSE>>=
library(splines)
if (!require(statmod)) install.packages('statmod')
library(statmod)
# number of evaluation points in each dimension 
num.x <- 10 
num.y <- 8 
num.z <- 4
# choose number of integration nodes and method 
num.int.z <- 50 
num.int.y <- 40 
int.z <- gauss.quad(n=num.int.z,kind="hermite")   
int.y <- gauss.quad(n=num.int.y,kind="hermite")
# select degree of splines 
degree <- 3 
# get spline knot vector defined on nodes 
xknots <- c(rep(0,times=degree),seq(0,10,le=num.x),rep(10,times=degree))  
zknots<-c(rep(min(int.z$nodes),times=degree),
seq(int.z$nodes[1],int.z$nodes[num.int.z],le=num.z),
rep(max(int.z$nodes),times=degree)) 
yknots<-c(rep(min(int.y$nodes),times=degree),
seq(int.y$nodes[1],int.y$nodes[num.int.y],le=num.y),
rep(max(int.y$nodes),times=degree))
# get grid points where to evaluate function
xdata <- as.array(seq(0,10,length=length(xknots)-degree-1)) 
ydata <- as.array(seq(int.y$nodes[1],int.y$nodes[num.int.y],length=length(yknots)-degree-1)) 
zdata <- as.array(seq(int.z$nodes[1],int.z$nodes[num.int.z],length=length(zknots)-degree-1))
# design matrices 
X <- splineDesign(knots=xknots,x=xdata) 
Y <- splineDesign(knots=yknots,x=ydata) 
Z <- splineDesign(knots=zknots,x=zdata)
DGP <- function( x,y,z ) { return((x + y - 5)^2 + (z-5)^2) } 
@


\subsection{Generate Data: Evaluate the function}

We compare evaluation time of the 3-dimensional function with a mapply
operation and with \textbf{RcppSimpleTensor}, as follows:

<<evaluate,tidy=FALSE>>=
data.grid <- expand.grid(x=xdata,y=ydata,z=zdata) 
trad.time<-system.time(
traditional<-array(with(data.grid,mapply(DGP,x,y,z)),
dim=c(length(xdata),length(ydata),length(zdata))))
# evaluate function with RcppSimpleTensor 
# define a tensor function to calculate function values: 
RcppVals <- tensorFunction( R[i,j,k] ~ (X[i] + Y[j] - 5)^2 + (Z[k] - 5)^2 ) 
# read: return array indexed by [i,j,k], defined as (x[i] + y[j] - 5)^2 + (z[k]-5)^2 
Rcpp.time <- system.time( RcppArray <- RcppVals(xdata,ydata,zdata) ) 
# check result
sum(abs(traditional - RcppArray)) 
# see timing
print(rbind(trad.time,Rcpp.time)) 
@


\subsection{Visualize the function}

Just for orientation, let's plot the function at the highest and lowest
value of variable z, respectively (see figure \ref{fig:function-DGP-on}).

\begin{figure}[H]
<<plotfun,par=TRUE,tidy=FALSE,echo=FALSE,fig.width=4,fig.height=4,out.width=0.45\linewidth,cache=FALSE>>=
zind <- 1:length(zdata); 
persp(x=xdata,y=ydata,z=RcppArray[,,tail(zind,1)],ticktype="detailed",xlab="x",ylab="y",zlab="value",theta=300,phi=30);
persp(x=xdata,y=ydata,z=RcppArray[,,head(zind,1)],ticktype="detailed",xlab="x",ylab="y",zlab="value",theta=300,phi=30);
@

\caption{function \texttt{DGP }on the evaluation grid at highest and lowest
point of $z$\label{fig:function-DGP-on}}
\end{figure}



\subsection{Compute Approximant on Grid}

Now we obtain the approximating coefficients $b$ by solving the system
\[
y=bA
\]
where $y$ are the function values at each grid point, $b$ is our
vector of coefficients and $A$ is the tensor product of spaces of
grid points $X$, $Y$ and $Z$.

<<get-coeffs>>=
b <- solve(kronecker(Z,kronecker(Y,X)), as.vector(RcppArray))
# put coefficients into a 3-dimensional array 
bb <- array(b,dim=c(length(xdata),length(ydata),length(zdata)))
@


\subsection{Use RcppSimpleTensor to Evaluate Spline}

Next, we generate random data in the domain of our approximation,
and evaluate the following expression, which is our approximant $\hat{f}$
at arbitrary data $(x,y,z)$:
\[
\hat{f}(x,y,z)=\sum_{i}\sum_{j}\sum_{k}b_{i,j,k}X_{i}(x)Y_{j}(y)Z_{k}(z)
\]
where $X_{i},Y_{j},Z_{k}$ are the values of the i-th, j-th and k-th
basis functions at the values $x,y,z$, respectively.

<<eval-spline-new,tidy=FALSE>>=
new_xdata <- sort( runif(n=30,min=min(xdata),max=max(xdata)) ) 
new_ydata <- sort( runif(n=25,min=min(int.y$nodes),max=max(int.y$nodes) ) ) 
new_zdata <- sort( runif(n=21,min=min(int.z$nodes),max=max(int.z$nodes) ) ) 
# basis for new values 
new_X <- splineDesign(knots=xknots,x=new_xdata) 
new_Y <- splineDesign(knots=yknots,x=new_ydata) 
new_Z <- splineDesign(knots=zknots,x=new_zdata) 
# define RcppSimpleTensor function 
spline.eval <- tensorFunction(R[nx,ny,nz] ~ coeffs[mx,my,mz] * Xbase[nx,mx] * Ybase[ny,my] * Zbase[nz,mz])
pred.vals <- spline.eval( bb, new_X, new_Y, new_Z )        
# or simply with inline: 
TIpred.vals <- TI( bb[m1,m2,m3] * new_X[n1,m1] * new_Y[n2,m2] * new_Z[n3,m3], n1+n2+n3)
@

Let's see how we are doing by plotting our approximant, again for
the two most extreme values of z:

<<plot-new,par=TRUE,tidy=FALSE,echo=FALSE,fig.width=4,fig.height=4,out.width=0.45\linewidth,cache=FALSE>>=
zind <- 1:length(new_zdata);
persp(x=new_xdata,y=new_ydata,z=pred.vals[,,head(zind,1)],ticktype="detailed",xlab="new_x",ylab="new_y",zlab="approx",theta=300,phi=30) 
persp(x=new_xdata,y=new_ydata,z=pred.vals[,,tail(zind,1)],ticktype="detailed",xlab="new_x",ylab="new_y",zlab="approx",theta=300,phi=30)
@

\pagebreak{}


\subsection{Perform numerical Integration}

Here we calculate the basis on the entire set of integration nodes,
and then we multiply by quadrature weights and sum over the correct
indices. We want to approximate
\[
E_{YZ}\left[f(x,y,z)\right]=\int f(x,y,z)s(z)g(y)dzdy
\]
by
\[
\hat{E}_{YZ}\left[f(x,y,z)\right]=\sum_{i}\sum_{j}\omega_{i}^{y}\omega_{j}^{z}\hat{f}(x,\tilde{y_{i}},\tilde{z_{j}})
\]
where $\left(\omega^{y},\omega^{z},\tilde{y},\tilde{z}\right)$ are
quadrature weights and nodes for $y,z$ respectively.

<<get-intdata,par=TRUE,tidy=FALSE,fig.width=4,fig.height=4,out.width=0.5\linewidth,cache=FALSE>>=
Int.base.z <- splineDesign(knots=zknots,x=int.z$nodes) 
Int.base.y <- splineDesign(knots=yknots,x=int.y$nodes) 
# evaluate spline with RcppSimple
Intdata <- spline.eval( bb, X, Int.base.y, Int.base.z )
yweights <- int.y$weights 
zweights <- int.z$weights
#### integrate
#i.e. weighted sum over corresponding dimensions 
RcppIntFun <- tensorFunction( R[nx] ~ Data[nx,ny,nz] * yweight[ny] * zweight[nz] ) 
Int.y.z <- RcppIntFun( Intdata, yweights, zweights )
# or inline
TI.Int.y.z <- TI( Intdata[nx,ny,nz] * yweights[ny] * zweights[nz], nx ) 
# compare
sum(Int.y.z - TI.Int.y.z)
dim(Int.y.z) 
# plot integrated function
plot(xdata,Int.y.z,type="l")
@

If we want to integrate only one out of many dimensions, like in 
\[
\hat{E}_{Y}\left[f(x,y,z)\right]=\sum_{i}\omega_{i}^{y}\hat{f}(x,\tilde{y_{i}},z)
\]
or 
\[
\hat{E}_{Z}\left[f(x,y,z)\right]=\sum_{j}\omega_{j}^{z}\hat{f}(x,y,\tilde{z_{j}})
\]
this is very easily accomplished with RcppSimpleTensor -- just select
the right dimensions and recompute:

<<one-only,echo=TRUE,tidy=FALSE>>=
# integrate w.r.t. z only  
Int.z <- TI( Intdata[nx,ny,nz] * zweights[nz], nx + ny) 
dim(Int.z) 
# integrate w.r.t. y only  
Int.y <- TI( Intdata[nx,ny,nz] * yweights[ny], nx + nz ) 
dim(Int.y)
@

We can again inspect the result graphically:

<<plot-last,par=TRUE,echo=FALSE,tidy=FALSE,fig.width=4,fig.height=4,out.width=0.45\linewidth,cache=FALSE>>=
persp(x=xdata,y=int.y$nodes,z=Int.z,xlab="xdata",ylab="int.y$nodes",zlab="Value",theta=300,phi=30)
persp(x=xdata,y=int.z$nodes,z=Int.y,xlab="xdata",ylab="int.z$nodes",zlab="Value",theta=300,phi=30)
@
\end{document}
